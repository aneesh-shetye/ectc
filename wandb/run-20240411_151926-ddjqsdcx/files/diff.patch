diff --git a/ectc.py b/ectc.py
index 98e94a8..12880ce 100644
--- a/ectc.py
+++ b/ectc.py
@@ -6,12 +6,13 @@ class ContextEnhance(nn.Module):
     
     def __init__(self, 
                  encoder, #an encoder model  
-                 pooling, # a pooling funciton to aggregate encoder outputs
+                 #pooling, # a pooling funciton to aggregate encoder outputs
                  proj: nn.Module 
                  ): 
 
+        super().__init__()
         self.encoder = encoder 
-        self.pooling = pooling 
+        #self.pooling = pooling 
         self.proj = proj 
     
     def forward(self,
@@ -22,8 +23,8 @@ class ContextEnhance(nn.Module):
        #tgt.shape = (batch_size, seq_len)
        batch_size = src.shape[0]
 
-       ws = self.encoder(src) 
-       wt = self.encoder(tgt)
+       ws = self.encoder(src)['last_hidden_state'] 
+       wt = self.encoder(tgt)['last_hidden_state']
        #ws.shape = wt.shape = (batch_size, seq_len, emb_dim)
 
        sigmas = torch.sum(ws, dim=1)
@@ -40,4 +41,4 @@ class ContextEnhance(nn.Module):
        #C.shape = (emb_dim2, emb_dim2)
        
        return c 
-       
\ No newline at end of file
+       
diff --git a/pretrain_ectc.py b/pretrain_ectc.py
index 012e7a0..d900353 100644
--- a/pretrain_ectc.py
+++ b/pretrain_ectc.py
@@ -34,7 +34,6 @@ torch.backends.cudnn.deterministic = True
 
 parser = argparse.ArgumentParser(description='ectc')
 
-parser = argparse.ArgumentParser(description='slot_vqa')
 
 #region: system config: 
 parser.add_argument('--workers', default=5, type=int, metavar='N', 
@@ -55,9 +54,9 @@ parser.add_argument('--start_epoch',default=0, type=int,
         help='start epoch of the pretraining phase')
 
 #region: training hyperparameters: 
-parser.add_argument('--epochs', default=30, type=int, metavar='N',
+parser.add_argument('--epochs', default=10, type=int, metavar='N',
                     help='number of total epochs to run')
-parser.add_argument('--batch_size', default=2, type=int, metavar='n',
+parser.add_argument('--batch_size', default=64, type=int, metavar='n',
                     help='mini-batch size')
 parser.add_argument('--learning_rate', default=1e-5, type=float, metavar='LR',
                     help='base learning rate')
@@ -76,14 +75,33 @@ parser.add_argument('--optimizer', default='adam', type=str, metavar='OP',
                     help='selecting optimizer')
 #endregion
 
+#region: transformer hyperparameters: 
+
+parser.add_argument('--proj_dim', default=512, type=int, metavar='D',
+                    help='output dimension of the projection layer')
+
 #region: tokenizer
 parser.add_argument('--text_encoder', default='bert-base-multilingual-uncased', type=str, metavar='T',
                     help='text encoder')
+parser.add_argument('--mbert_out_size', default=768, type=int, metavar='S',
+                    help='dimension of encoder (mbert) output')
 #endregion
 
 
 args = parser.parse_args()
 
+def tensor2img(img): 
+    #inp.shape = 3, H, W
+
+    #test_img = img.permute(1,2,0)
+    test_img = img.unsqueeze(-1)
+    test_img = test_img.cpu().detach().numpy()
+    test_img = 255*test_img
+    test_img = test_img.astype(np.uint8)
+    return test_img
+#endregion
+
+
 #initializing the tokenizer and dataset
 tokenizer = AutoTokenizer.from_pretrained(args.text_encoder)
 
@@ -106,19 +124,17 @@ def pretrain(gpu, args):
     
     if args.rank == 0: 
 
-        '''
         wandb.init(config=args, project='ectc')
         wandb.run.name = f"pc-dry-run-{wandb.run.id}"
         wandb.config.update(args)
         config = wandb.config
         wandb.run.save()
-        '''
 
         checkpoint_dir = args.root.joinpath(args.checkpoint_dir)
         if not os.path.exists(checkpoint_dir): 
             os.makedirs(checkpoint_dir)
         
-        stats_file = open(checkpoint_dir / '')
+        #stats_file = open(checkpoint_dir / 'stats_file.txt')
 
     torch.cuda.set_device(gpu)
     torch.backends.cudnn.benchmark = True
@@ -127,19 +143,19 @@ def pretrain(gpu, args):
     # dataset =  load_dataset("stas/wmt14-en-de-pre-processed")
 
     #load dataset
-    dataset = dataset("bentrevett/multi30k")
+    dataset = load_dataset("bentrevett/multi30k")
 
     assert args.batch_size%args.world_size == 0 
     per_device_batch_size = args.batch_size // args.world_size
 
     train_sampler = torch.utils.data.distributed.DistributedSampler(dataset['train'])
     train_loader = DataLoader(dataset['train'], batch_size=per_device_batch_size, 
-                              shuffle=True, num_workers=args.workers, pin_memory=True, sampler=train_sampler, 
+                              shuffle=False, num_workers=args.workers, pin_memory=True, sampler=train_sampler, 
                               collate_fn=MyCollate(tokenizer))
 
     
     #model to pretrain
-    encoder = BertModel.from_pretrained(args.text_encoder)
+    encoder = BertModel.from_pretrained(args.text_encoder).cuda(gpu)
 
     checkpoint = os.path.join(checkpoint_dir, "enc_ckpt.pth")
     if args.load and os.path.exists(checkpoint): 
@@ -148,15 +164,17 @@ def pretrain(gpu, args):
         encoder.load_state_dict(ckpt['model'])
         print("Model Loaded")
 
-    encoder.cuda(gpu)
+    encoder = encoder.cuda(gpu)
 
-    proj_layer = nn.Linear(args.mbert_out_size, args.proj_dim).cuda(gpu)
+    proj_layer = nn.Linear(args.mbert_out_size, args.proj_dim).cuda(gpu) 
     pretrainer = ContextEnhance(encoder, proj_layer).cuda(gpu)
 
     if args.optimizer == 'adam': 
-        optimizer = torch.optim.Adam(lr=args.learning_rate)
+        optimizer = torch.optim.Adam(pretrainer.parameters(), lr=args.learning_rate)
     else: 
-        optimizer = torch.optim.SGD(lr=args.learning_rate)
+        optimizer = torch.optim.SGD( pretrainer.parameters(), lr=args.learning_rate)
+
+
     
 
     for epoch in range(args.start_epoch, args.epochs): 
@@ -170,30 +188,39 @@ def pretrain(gpu, args):
         
         for step, item in enumerate(train_loader, start=epoch*len(train_loader)): 
             
-            src = item['eng_batch'] 
-            tgt = item['de_batch']
+            src = item[0] 
+            tgt = item[1]
 
-            src.cuda(gpu)
-            tgt.cuda(gpu)
+            src = src.cuda(gpu)
+            tgt = tgt.cuda(gpu)
 
             optimizer.zero_grad()
 
             corr = pretrainer(src, tgt)            
+            
+            img = tensor2img(corr[0]) 
+
+            img = wandb.Image(img, caption="corr_mat") 
+            wandb.log({'image': img})  
 
             on_diag = torch.diagonal(corr).add_(-1).pow_(2).sum()
             off_diag = off_diagonal(corr).pow_(2).sum()
             loss = on_diag + args.lambd * off_diag
 
-            loss.backwards()
+            wandb.log({"iter loss": loss})
+            loss.backward()
             torch.nn.utils.clip_grad_norm_(pretrainer.parameters(), args.clip)
             optimizer.step()
-
-            epoch_loss += loss.item()
             counter += 1
 
+        epoch_loss += loss.item()
+        epoch_loss = epoch_loss/counter
+        wandb.log({"epoch loss": loss})
+
     
+if __name__ == '__main__': 
+    main()
+    wandb.finish()
+
 
 
-    
-    
-    
diff --git a/utils/dataloader_utils.py b/utils/dataloader_utils.py
index 079cbb2..64dfab2 100644
--- a/utils/dataloader_utils.py
+++ b/utils/dataloader_utils.py
@@ -10,6 +10,7 @@ class MyCollate():
         eng = [item['en'].lower() for item in batch] 
         de = [item['de'].lower() for item in batch] 
 
+        self.tokenizer = self.tokenizer
         eng_batch = self.tokenizer(eng, max_length=256, 
                                    padding='max_length', 
                                    truncation=True, 
